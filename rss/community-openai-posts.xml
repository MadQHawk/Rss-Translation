<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>OpenAI 开发者论坛 - 最新帖子</title>
    <link>https://community.openai.com</link>
    <description>最新帖子</description>
    <lastBuildDate>Thu, 25 Apr 2024 03:19:42 GMT</lastBuildDate>
    <item>
      <title>助理无法再访问通过 API 上传的文件</title>
      <link>https://community.openai.com/t/assistant-no-longer-able-to-access-files-uploaded-via-the-api/694352#post_4</link>
      <description><![CDATA[这里有同样的问题。只需设置整个上传文件，然后在创建运行时附加到消息......它所做的就是搜索已附加到助手的整个矢量存储区，而不仅仅是附加到消息的文件。
结果非常昂贵，而且不太理想。
它会产生文件内容的幻觉，直到我提到所附文档的标题，然后它就会神奇地知道所有内容。
从文档中可以看出，向带有附件的线程添加消息会神奇地创建矢量存储，解析文件，然后在流式传输时准备好......除非要求它立即准备好可能太过分了？ ]]></description>
      <guid>https://community.openai.com/t/assistant-no-longer-able-to-access-files-uploaded-via-the-api/694352#post_4</guid>
      <pubDate>Thu, 25 Apr 2024 02:48:10 GMT</pubDate>
    </item>
    <item>
      <title>GPT-4 定价的当前使用上限</title>
      <link>https://community.openai.com/t/current-usage-cap-for-gpt-4-pricing/523444?page=2#post_34</link>
      <description><![CDATA[转到他们的 YouTube 频道或撰写 YouTube 评论，他们现在或之前都没有采取任何措施，他们应该解雇之前被解雇的人，并招募一些知道他们在做什么的新人]]></description>
      <guid>https://community.openai.com/t/current-usage-cap-for-gpt-4-pricing/523444?page=2#post_34</guid>
      <pubDate>Thu, 25 Apr 2024 02:47:25 GMT</pubDate>
    </item>
    <item>
      <title>GPT 无法访问或读取 PDF</title>
      <link>https://community.openai.com/t/gpt-not-accessing-or-reading-pdfs/726992#post_2</link>
      <description><![CDATA[从我的 GPT...“因为我无法直接访问外部 API”...发生了什么事？]]></description>
      <guid>https://community.openai.com/t/gpt-not-accessing-or-reading-pdfs/726992#post_2</guid>
      <pubDate>Thu, 25 Apr 2024 02:44:26 GMT</pubDate>
    </item>
    <item>
      <title>＃功能。 #multi_tool_use 来自轻微微调的 gpt 3.5 1106</title>
      <link>https://community.openai.com/t/functions-multi-tool-use-from-lightly-fine-tuned-gpt-3-5-1106/720694#post_5</link>
      <description><![CDATA[我可能沟通错误，请允许我解释一下
我目前不使用助手前端
我使用聊天补全…具体来说，我在 3.5 0125 和 1106 之间切换
我通过上传 jsonl 文件进行训练
例如，我将工具调用放入 jsonl 中，如下所示
{
“角色”：“助理”，
“内容”：“”，
“工具调用”：{
“tool_call_id”：“再见，没有问题”，
“参数”：{}
},
“权重”：1
},
{
“角色”:“系统”,
“content”: “我们已经结束对话了”
}
然后…由于openai不支持微调工具语言，我们必须使用已弃用的函数调用
我使用脚本将所有工具调用转换为函数调用
如果消息中有“tool_call”：
tool_call = message.pop(&#39;tool_call&#39;)
函数调用 = {
‘名称’: tool_call[‘tool_call_id’],
‘参数’: json.dumps(tool_call[‘params’])
}
消息[&#39;function_call&#39;] = function_call
这适用于创建一个 jsonl 文件，其函数如下所示
{
“角色”：“助理”，
“内容”：“”，
“权重”：1，
“函数调用”：{
“name”：“再见，没有问题”，
“参数”：“{}”
}
},
{
“角色”:“系统”,
“content”: “我们已经结束对话了”
}]]></description>
      <guid>https://community.openai.com/t/functions-multi-tool-use-from-lightly-fine-tuned-gpt-3-5-1106/720694#post_5</guid>
      <pubDate>Thu, 25 Apr 2024 02:41:15 GMT</pubDate>
    </item>
    <item>
      <title>Beta V2 中的助手在注释中调用其他助手 file-id</title>
      <link>https://community.openai.com/t/assistant-in-beta-v2-calls-other-assistant-file-id-in-annotation/726880#post_9</link>
      <description><![CDATA[线程的单个向量存储跟随线程到在其上运行的任何助手，这是有道理的。
文档中的特定部分可能会发挥作用，但其背后的“原因”并未列出。
&lt;块引用&gt;
当矢量存储过期时，该线程上的运行将失败。要解决此问题，您只需使用相同的文件重新创建一个新的 vector_store 并将其重新附加到线程即可。

可能是线程的“file_search”：{“vector_store_ids”…}引用了不再存在的特定ID，并且它们使AI不会继续默默地处理此错误。他们告诉您用新 ID 替换 ID，并且通过修改线程删除 ID 也应该允许您继续，我认为。
&lt;小时/&gt;
我认为这里的实际体验是：

助理 AI 模型在拥有很长的上下文或已经看到一个搜索结果后，不喜欢继续使用当前工具进行搜索；
没有关于 switcharoo 的信息让 AI 认为可能出现不同的结果；
如果完全删除文件搜索及其注入的工具规范，人工智能将无法深入了解文档文本是如何到达那里的。

也许当我有 1 美元的问题时我会玩它。]]></description>
      <guid>https://community.openai.com/t/assistant-in-beta-v2-calls-other-assistant-file-id-in-annotation/726880#post_9</guid>
      <pubDate>Thu, 25 Apr 2024 02:38:54 GMT</pubDate>
    </item>
    <item>
      <title>创建抵押数据库/帮助台</title>
      <link>https://community.openai.com/t/creating-a-mortgage-database-helpdesk/727033#post_1</link>
      <description><![CDATA[您好，我希望为我们的网站建立一个服务台，用户可以在其中查询抵押贷款指南、我们使用的公司贷款机构、产品知识。这一习惯让我们工作得相当好，但我想做一个可以嵌入我们的网站的。我看到的问题是我已经上传了大约 5k 页的信息，需要检查，似乎助手版本有所不同，并且可能要花很多钱才能做到这一点？
解决这个问题的最佳方法是什么，非常愿意接受有关如何到达所选端点的建议。]]></description>
      <guid>https://community.openai.com/t/creating-a-mortgage-database-helpdesk/727033#post_1</guid>
      <pubDate>Thu, 25 Apr 2024 02:32:59 GMT</pubDate>
    </item>
    <item>
      <title>助理基准和评估框架</title>
      <link>https://community.openai.com/t/benchmark-evaluation-frameworks-for-assistants/727031#post_1</link>
      <description><![CDATA[一段时间以来，我一直在寻找适合 OpenAI 助手的 LLM 评估框架。我最喜欢的（到目前为止）是deepeval，但我仍在寻找。
一些问题：

是否有人在使用某些框架评估其 Google 助理时取得过成功和/或好的示例？如果是的话，您能分享一下编码示例吗？
如果您在生产中有助理，是否有 CI/CD 流程来根据助理的响应评估用户输入以及评估频率？

感谢您的帮助。]]></description>
      <guid>https://community.openai.com/t/benchmark-evaluation-frameworks-for-assistants/727031#post_1</guid>
      <pubDate>Thu, 25 Apr 2024 02:30:11 GMT</pubDate>
    </item>
    <item>
      <title>＃功能。 #multi_tool_use 来自轻微微调的 gpt 3.5 1106</title>
      <link>https://community.openai.com/t/functions-multi-tool-use-from-lightly-fine-tuned-gpt-3-5-1106/720694#post_4</link>
      <description><![CDATA[JSONL 是一个每行都有一个新 JSON 的 JSON 列表，用于微调或批处理，不会通过助手检索工具的上传验证器（该工具解析内部文件并执行检查并根据情况拒绝它们）在假定的文件扩展名上），并且简单地说不是受支持的文件类型 - API 文档 - 支持的文件。
你不能在助手中使用 -1106 微调，所以我不明白这有什么关系。
这与最初的担忧无关，即在使用聊天完成端点时，经过微调的模型无法正确使用并行工具。您可以提供的调整会降低此内部工具的质量，因为您无法训练它们的使用，只能训练其他类型的响应。]]></description>
      <guid>https://community.openai.com/t/functions-multi-tool-use-from-lightly-fine-tuned-gpt-3-5-1106/720694#post_4</guid>
      <pubDate>Thu, 25 Apr 2024 02:24:16 GMT</pubDate>
    </item>
    <item>
      <title>解释 GPT4 Vision 在多标签分类中生成的分数</title>
      <link>https://community.openai.com/t/interpret-scores-generated-by-gpt4-vision-in-multilabel-classification/727000#post_2</link>
      <description><![CDATA[我将在这里回答一些更具智力刺激的问题 - 你需要做艰苦的工作。



夜煞：
&lt;块引用&gt;
如何使生成的输出可重现


输出只能大部分重现。模型本身在相同的运行之间存在差异。
然后，人工智能模型使用采样器从概率中随机挑选令牌。如果问题的可能第一个标记有 80% 确定是“猫”，15% 确定是“狗”，那么在默认 API 采样参数下，15% 的试验将让 AI 回答“狗”。
top_p 是最能让您的 AI 只回答所绘制路径的参数。当 10% 为 0.1 时，仅从占据前 10% 的那些标记生成响应 - AI 只能写“cat”。这可能就是您想要的 - 最佳答案。
seed 几乎每次都会给你“狗”，如果它是由特定种子选择的话，其中可重现部分会重用相同的随机性。



夜煞：
&lt;块引用&gt;
在提示中，还可以要求它生成概率分数


不过，这些都近乎幻觉。如果你问人工智能它的确定性如何，你将获得与训练和提示相关的分数。
考虑一下我的系统消息是否告诉人工智能有关其自身的信息之一：

“人工智能是一位拥有超人类逻辑和推理能力的专家”
“ChatPro 只给出正确答案”
“人工智能语言生成可能会犯错误，因此请仔细检查您所写的内容”

如果您要求 AI 评估自身，而不是底层机制（AI 无法观察到），此类其他输入上下文可以使选择为概率的标记“更加自信”。



夜煞：
&lt;块引用&gt;
应该如何解释生成的概率分数


了解确定性内部运作的最佳方法是使用 logprobs。在复杂的答案中，您必须在格式化的答案中导航并找到相关的标记。
可能使用此技术的示例。 “以 JSON（格式）对本书评进行评分，从 1 到 10，从 1：非常负面，到 10：非常正面”。您可以获取答案位置处的前 20 个对数概率，从中提取所有数字标记，然后找到概率质量的中值或加权概率的平均值。然后，您可能需要重新规范化答案范围，以便最积极和最消极的数字仍然可以在 1 到 10 之间。
这也应该能让您深入了解您的其他问题。实际的任务，您需要保持相对简单和良好的指导，确保人工智能知道要生成什么以及在生成输出时正在回答什么输入。]]></description>
      <guid>https://community.openai.com/t/interpret-scores-generated-by-gpt4-vision-in-multilabel-classification/727000#post_2</guid>
      <pubDate>Thu, 25 Apr 2024 02:15:14 GMT</pubDate>
    </item>
    <item>
      <title>Beta V2 中的助手在注释中调用其他助手 file-id</title>
      <link>https://community.openai.com/t/assistant-in-beta-v2-calls-other-assistant-file-id-in-annotation/726880#post_8</link>
      <description><![CDATA[经过更多测试后，我发现了更多相关结果，进一步说明了这一点。
如果将第二个助手添加到包含来自另一个助手向量存储 file_search 的块的线程，则第二个助手似乎总是说它找不到相关数据或引用来自第一个助手的块。
即使提示第二个助手仅搜索相关向量存储，它似乎也永远无法从其向量存储中检索块。在这些情况下，不会返回任何相关数据的响应。其他情况似乎在响应中显示了正确的上下文（很可能是由于系统提示），但注释始终来自第一个助手的块。
此外，作为我关于将 file_ids 加载到线程向量存储的声明的后续内容是正确的。当线程中所有助手的所有相关文件都存储在单个向量存储中时，每个助手将返回包含来自其指定代码/标准/手册的相关数据的响应]]></description>
      <guid>https://community.openai.com/t/assistant-in-beta-v2-calls-other-assistant-file-id-in-annotation/726880#post_8</guid>
      <pubDate>Thu, 25 Apr 2024 02:09:05 GMT</pubDate>
    </item>
    <item>
      <title>助理无法再访问通过 API 上传的文件</title>
      <link>https://community.openai.com/t/assistant-no-longer-able-to-access-files-uploaded-via-the-api/694352#post_3</link>
      <description><![CDATA[我遇到了类似的问题，当我上传文件时，输出似乎表明只能访问或上传文件的一部分。当我检查文件大小时，它似乎是正确的。例如，如果我上传一个 C 源文件并对其进行分析，返回的消息似乎表明仅上传或可访问一个片段。这是由于我不知道的某些文件大小限制造成的吗？]]></description>
      <guid>https://community.openai.com/t/assistant-no-longer-able-to-access-files-uploaded-via-the-api/694352#post_3</guid>
      <pubDate>Thu, 25 Apr 2024 02:07:26 GMT</pubDate>
    </item>
    <item>
      <title>用于在网站上渲染 Assistant API 的 UI（首选低代码）</title>
      <link>https://community.openai.com/t/ui-for-assistants-api-rendering-on-websites-low-code-preferred/727021#post_1</link>
      <description><![CDATA[演示我在纯原始编码之外的网站上构建的 Assistants API 的最佳方式是什么？
流线型？颤振流？微网？网络流？思维工作室？语音流？
建议？有推荐吗？]]></description>
      <guid>https://community.openai.com/t/ui-for-assistants-api-rendering-on-websites-low-code-preferred/727021#post_1</guid>
      <pubDate>Thu, 25 Apr 2024 02:04:52 GMT</pubDate>
    </item>
    <item>
      <title>删除文件时出错：没有这样的文件对象：file-xxxx：重新出现</title>
      <link>https://community.openai.com/t/there-was-an-error-deleting-the-file-no-such-file-object-file-xxxx-reappeared/727018#post_1</link>
      <description><![CDATA[2024 年 4 月 Assistant API 更新后，此问题再次出现。
 ]]></description>
      <guid>https://community.openai.com/t/there-was-an-error-deleting-the-file-no-such-file-object-file-xxxx-reappeared/727018#post_1</guid>
      <pubDate>Thu, 25 Apr 2024 02:04:10 GMT</pubDate>
    </item>
    <item>
      <title>如何降低Text-To-Speech API的延迟？</title>
      <link>https://community.openai.com/t/how-to-decrease-the-latency-of-text-to-speech-api/727015#post_1</link>
      <description><![CDATA[大家好，
就上下文而言，我正在使用 GPT4 API 执行文本转语音，每当我传入大量文本时，延迟可能达到近一分钟。我想知道是否有办法减少延迟？
到目前为止我发现了什么。在常见问题解答 (https://help.openai.com/en/articles/8555505-tts-api) 中，他们提到我们可以通过设置 stream=True 来分块传输音频。
我已尝试使用以下代码：
response = self.client.audio.speech.create(
model=“tts-1”,
voice=“shimmer”,
input=msg,
stream=True)
但我收到此错误：
TypeError：Speech.create() 获得了意外的关键字参数‘stream’
所以我有点困惑，为什么他们网站上说“stream”存在，却说它不存在？
任何帮助都非常感谢 ]]></description>
      <guid>https://community.openai.com/t/how-to-decrease-the-latency-of-text-to-speech-api/727015#post_1</guid>
      <pubDate>Thu, 25 Apr 2024 02:01:46 GMT</pubDate>
    </item>
    <item>
      <title>内存更新后代码响应严重滞后</title>
      <link>https://community.openai.com/t/severe-lag-in-code-responses-after-memory-update/727014#post_1</link>
      <description><![CDATA[代码示例无法完成，然后在提示继续时出错。
]]></description>
      <guid>https://community.openai.com/t/severe-lag-in-code-responses-after-memory-update/727014#post_1</guid>
      <pubDate>Thu, 25 Apr 2024 02:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>